# Отчёт (Семестр 2, СЗ_1): ML API + LLM API + тесты + CI + агент

## Введение
В рамках задания семестра 2 реализован полностью локальный проект на Python 3.10+, включающий:
- REST API для ML-модели (анализ тональности RU текста) на FastAPI,
- REST API для локальной LLM через Ollama,
- модульные тесты PyTest,
- CI pipeline GitHub Actions,
- прототип ИИ-агента на LangChain,
- сравнение нескольких LLM (2–8B) по качественным и инфраструктурным критериям.

Ограничения: используются только предобученные модели, без fine-tuning, с приоритетом CPU-first.

---

## Часть 1 — API для ML-модели
### 1. Архитектура
Слой приложения разделён на:
- `app/main.py` — FastAPI роуты и обработка ошибок,
- `app/schemas.py` — Pydantic схемы запрос/ответ,
- `app/model.py` — доступ к `transformers.pipeline` и инференс.

### 2. Почему FastAPI
- Высокая производительность (ASGI)
- Автоматическая документация Swagger/OpenAPI
- Валидация входных данных через Pydantic
- Удобство тестирования через TestClient

### 3. Endpoint /predict
`POST /predict` принимает JSON:
```json
{ "text": "пример текста" }
```
и возвращает:
```json
{ "label": "POSITIVE", "score": 0.98 }
```

Обработка ошибок:
- пустой `text` -> 400,
- ошибки инференса -> 500.

---

## Часть 2 — Тесты и CI
### 1. Зачем нужны тесты
Тесты позволяют:
- автоматически проверять корректность API после изменений,
- ловить регрессии,
- ускорять разработку за счёт быстрого фидбэка.

### 2. PyTest + TestClient
Тесты проверяют:
- статус 200 на валидном запросе,
- наличие полей `label`/`score`,
- обработку пустого текста.

Для LLM endpoint `/generate` используется мок (monkeypatch), чтобы CI не зависел от работы Ollama.

### 3. CI/CD через GitHub Actions
Workflow `.github/workflows/tests.yml` запускает `pytest` при каждом push и pull request на Python 3.10.

---

## Часть 3 — API для LLM (Ollama)
### Endpoint /generate
`POST /generate` принимает:
```json
{ "prompt": "Объясни, что такое API", "model": "mistral:7b" }
```
и возвращает:
```json
{ "response": "..." }
```

Реализация использует HTTP API Ollama (`/api/generate`).

Обработка ошибок:
- если Ollama не запущена -> 503,
- прочие ошибки -> 500.

---

## Часть 4 — Сравнение LLM
Сравнение вынесено в `llm_comparison/comparison.md`.

Критерии:
- логика,
- фактическая точность,
- галлюцинации,
- устойчивость к провокациям,
- скорость,
- потребление RAM.

Вывод (пример): для агента чаще выбирают `mistral:7b`, так как она даёт лучшее качество, но требует больше ресурсов.

---

## Часть 5 — Агент (LangChain)
### ТЗ
ТЗ агента описано в `agent/agent_task.md`.

### Реализация
`agent/agent.py` реализует:
- `PromptTemplate` (строгий формат ответа),
- `LLMChain`,
- память (`ConversationBufferMemory`).

### Метрики оценки агента
- доля корректно сформированных структурированных ответов,
- качество рекомендаций (экспертная оценка),
- устойчивость к prompt injection (количество успешных атак),
- время ответа.

### Уязвимости и способы защиты
1) **Prompt injection**:
- фиксированный системный промпт,
- ограничение формата ответа,
- фильтрация входа (политики).

2) **Галлюцинации**:
- требование указывать неопределённость,
- проверка фактов (при наличии внешних источников),
- ограничение домена задач.

3) **Отсутствие валидации**:
- Pydantic схемы,
- проверка пустых строк,
- лимиты длины запросов.

---

## Заключение
Проект демонстрирует полный цикл разработки: от инференса предобученной ML-модели и локальной LLM до тестирования, CI и прототипа агента.

