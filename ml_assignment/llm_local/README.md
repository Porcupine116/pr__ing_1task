# Локальная LLM (7–8B параметров) через Ollama

> Если в Windows PowerShell русский текст отображается «кракозябрами», включите UTF-8:
> ```powershell
> chcp 65001
> ```

В задании требуется **развернуть локальную LLM (7–8B параметров)** и показать пример её запуска.

Важно: в рамках этого проекта мы **не обучаем** LLM, а используем готовую модель в режиме inference.

---

## Почему Ollama
**Ollama** — удобный способ запускать большие языковые модели локально:
- автоматически скачивает нужные веса,
- простой CLI,
- работает локально (без API и без фронтенда),
- может использовать CPU (медленно) и GPU (если доступно).

Официальный сайт: https://ollama.com/

---

## Рекомендуемая модель (7B)
Один из самых популярных вариантов для учебной демонстрации:
- **Mistral 7B Instruct** (7B параметров)

В Ollama обычно доступна как:
- `mistral:7b-instruct`

---

## Установка Ollama (Windows)
1. Скачайте и установите Ollama: https://ollama.com/download
2. Проверьте установку в терминале:

```bash
ollama --version
```

---

## Запуск модели (интерактивный режим)
Первый запуск скачает модель (может занять время).

```bash
ollama run mistral:7b-instruct
```

Далее введите запрос, например:
- `Объясни простыми словами, что такое градиентный спуск.`

---

## Запуск одной командой (без интерактива)

```bash
ollama run mistral:7b-instruct "Сгенерируй 5 идей проектов по машинному обучению для студента."
```

---

## Мини-демо под задачи курса (пример промпта)

```bash
ollama run mistral:7b-instruct "Ты — ассистент по ML. Кратко объясни разницу между Transformer и CNN."
```

---

## Ограничения и ресурсы
- 7B модели требуют заметного объёма RAM. На CPU запуск возможен, но будет ощутимо медленнее.
- Если памяти не хватает, можно временно использовать более компактную модель (например 3B), но в отчёте указать, что требование 7B выполняется на машине с достаточной RAM.

---

## Что сдавать по пункту LLM
- Скриншот/лог команды `ollama run mistral:7b-instruct`.
- Пример 1–2 запросов и ответов модели.
